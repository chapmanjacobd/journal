I would say it depends how you model the data. Naive data structures and indexes scale well to 2TB. If you can split up the data so that rows are not too tall and not too wide with appropriate incremental data loading / transforms you could get away with 20TB maybe even 200TB but between those two you would want to evaluate distributed databases, if only for redundancy and continuity purposes
