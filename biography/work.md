Experienced data engineer, spatial analyst, and linux systems admin. Strong information technology professional with a background in anthropology.

Available for consulting:
https://www.moonlightwork.com/app/users/9511/profile

Competencies:

- Database design, SQL query optimization, PL/SQL
- Extract/Load, Transform (ETL / Data Pipelines)
- Process mapping / automation
- Functional Programming
- Bayesian Statistics, Principal Component Analysis
- Spreadsheets, Excel
- Batch processing, high-performance computing (HPC)

Tools:

- Defensive design, poka-yoke, pareto charts
- Python: geopandas, rasterio, seaborn, ipdb
- R: data.table, reshape2, caret, qualityTools (Six Sigma)
- PostgreSQL / PostGIS, GDAL, QGIS, OpenStreetMap
- dbt, GCP, BigQuery, rsync, GNU Parallel
- CentOS, Fedora, openSUSE

I enjoy building CLI tools to help teams work faster.

ML Pipelines 5/5 
I was often embedded into the ML team at Atlas AI to refactor and productionize Geospatial ML code. I wouldn't be comfortable training a model all by myself but I'm competent in ML Pipelines
 
Data Engineering, Schema & Storage 5/5 
I'm comfortable with databases. I've played around with PostGIS (with vector and raster), BigQuery, BigTable, and SQLite. I know what their strengths and weaknesses are. I have several years of experience with each of them except for BigTable.
 
Compute 5/5 
I'm comfortable with performance optimization and right-sizing programs with machine-types. I'm familiar with linux tools to find out what the system which can give hints to further optimize it (rewrite some code or change machine type to something that fits better to save money): strace, ltrace, iotop, PSI, instruction pipelining, GNU Parallel, etc
 
GPU 1/5 
I don't have much experience with ML training or GPU-powered inference. Maybe I've used it once or twice? but it was automatic and never appeared to be a bottleneck so I didn't look into what it was doing at a diagnostic level.
 
Orchestration & IaC, distributed compute 4/5 
I'm pretty familiar with Airflow and Argo Workflows for ML and Data Validation pipelines. For large-scale data processing I've mostly used Cloud Batch and some HashiCorp Nomad. I'm interested to see how you use Kubernetes. I've never used Flyte but I've used Dagster which seems somewhat similar... at least wrt putting python decorators everywhere. At Atlas AI I owned and operated the first six pipelines that used Cloud Batch. Two of those were Sentinel-2 and Landsat mosaicking which were thousands of VMs running simultaneously and several PB of data
